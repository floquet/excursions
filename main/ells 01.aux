\relax 
\@writefile{toc}{\contentsline {chapter}{Preface}{ix}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Life is complex}{ix}}
\@writefile{lot}{\contentsline {table}{\numberline {0.1}{\ignorespaces Matrix manipulations for $\A {*}$ and $\A {T}$.}}{ix}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Readings}{ix}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Least Squares}{ix}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Linear Algebra and Matrix Analysis}{x}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Numerical Linear Algebra}{x}}
\@writefile{toc}{\contentsline {section}{\numberline {0.6}Discussions on Least Squares}{x}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{I}Rudiments}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Least Squares Problems}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Linear Systems}{3}}
\newlabel{eq:axeb}{{1.1}{3}}
\newlabel{eq:basics}{{1.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}$ \left \delimiter 69645069    \A {}\tmspace  +\thinmuskip {.1667em}x - b  \right \delimiter 86422285  = 0$}{3}}
\newlabel{eq:axmb}{{1.1.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}$ \left \delimiter 69645069    \A {}\tmspace  +\thinmuskip {.1667em}x - b  \right \delimiter 86422285  > 0$}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Least Squares Solutions}{4}}
\newlabel{eq:simple case}{{1.3}{4}}
\newlabel{eq:xstar}{{1.4}{4}}
\newlabel{eq:simple case solution}{{1.5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The residual error $\left \delimiter 69645069 r \right \delimiter 86422285 _{2}$ given in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 1.5\hbox {}\unskip \@@italiccorr )}}.}}{5}}
\newlabel{fig:v graph}{{1.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Zonal Approximation}{5}}
\newlabel{ssec:zonal solution}{{1.2.1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Zonal Problem}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Scalar function $\phi $ and approximations.}}{6}}
\newlabel{fig:sticks}{{1.2}{6}}
\newlabel{eq:zonalls}{{1.6}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Zonal Solution}{6}}
\newlabel{ssec:modal approx}{{1.2.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Modal Approximation}{7}}
\newlabel{ssec:modal problem}{{1.2.2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Modal Problem}{7}}
\newlabel{ssec:modal solution}{{1.2.2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Modal Solution}{7}}
\newlabel{eq:modalls}{{1.7}{7}}
\newlabel{eq:column vectors}{{1.8}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Errors}{8}}
\newlabel{sec:lsp}{{1.3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Least Squares Problem}{8}}
\newlabel{eq:xlsdef}{{1.9}{8}}
\newlabel{eq:general soln}{{1.10}{8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Least Squares Solutions}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Fundamental Theorem of Linear Algebra}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The Fundamental Theorem of Linear Algebra}}{10}}
\newlabel{tab:ftola}{{2.1}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The Fundamental Theorem of Linear Algebra in pictures}}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Dimensions of the fundamental subspaces for $ \A {} \in \mathbb  {C}^{m \times n}_{\rho }$.}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Singular Value Decomposition\ - I}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}SVD Theorem}{13}}
\newlabel{eq:svd block}{{2.2.1}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Orthonormal spans for the invariant subspaces.}}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}SVD and Least Squares}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Unitary transformation}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Pseudoinverse solution}{14}}
\newlabel{eq:svd soln}{{2.2.2}{15}}
\newlabel{eq:mpptsvd}{{2.2.2}{15}}
\@writefile{toc}{\contentsline {subsubsection}{In retrospect}{15}}
\newlabel{eq:r2:a}{{2.3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Decomposing the data vector.}}{16}}
\newlabel{fig:decomposing data vector}{{2.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Singular Value Decomposition\ - II}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}$ \Sigma ^{\mathrm  {}} $ Gymnastics}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Fundamental Projectors}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Fundamental Projectors using the pseudoinverse.}}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Fundamental Projectors using domain matrices.}}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Projections of the data vector.}}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Least Squares Solution - Again}{19}}
\newlabel{eq:}{{2.4}{19}}
\newlabel{sec:reflection invariance}{{2.5}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Reflection Invariance}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Data vector resolved as $b = {\color {blue} {b_{ \mathcal  {R} }}} + {\color {red} {b_{ \mathcal  {N} }}}$.}}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Data vector resolved as $b + {\color {red} {b_{ \mathcal  {N} }}} = {\color {blue} {b_{ \mathcal  {R} }}}$.}}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces Reflecting the data for $\A {}x - T = r$.}}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The merit function for the learning curve showing the minimum and the value.}}{21}}
\newlabel{fig:learn:merit}{{2.5}{21}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Modal Example I}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Modal Approximation}{23}}
\newlabel{eq:lr trial}{{3.1}{23}}
\newlabel{eqn:merit}{{3.1}{23}}
\citation{Bevington}
\newlabel{eq:gradient lr}{{3.2}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bevington Example}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Measuring the temperature of a bar.}}{24}}
\newlabel{fig:bar}{{3.1}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Problem Statement}{24}}
\newlabel{sec:normal I}{{3.2.2}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Normal Equations via Calculus}{25}}
\newlabel{eq:bev pde}{{3.3}{25}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Problem statement for linear regression.}}{26}}
\newlabel{tab:bevington inputs}{{3.1}{26}}
\newlabel{eq:modal matrix eq}{{3.4}{26}}
\newlabel{eq:det}{{3.5}{26}}
\newlabel{eq:bevington matrix inverse}{{3.6}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Raw data and results.}}{27}}
\newlabel{tab:bevington data and results}{{3.2}{27}}
\newlabel{eqn:bevington solution product}{{3.2.2}{27}}
\newlabel{eqn:bevington soln}{{3.2.2}{27}}
\newlabel{eqn:bevington error terms}{{3.2.2}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Results for linear regression.}}{28}}
\newlabel{tab:bevington solution}{{3.3}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Numerical Results}{28}}
\newlabel{sec:exact form}{{3.3.1}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Exact Form}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Computed Form}{29}}
\newlabel{eq:soln vector}{{3.3.2}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Visualization}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Seeing the Solution}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Solution plotted against data with residual errors shown in red.}}{31}}
\newlabel{fig:bevington soln v data}{{3.2}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Scatter plot of residual errors.}}{31}}
\newlabel{fig:bevington residuals}{{3.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Scatter plot of residual errors with data points connected.}}{32}}
\newlabel{fig:bevington residuals line}{{3.4}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Seeing the Uncertainty}{32}}
\newlabel{fig:bevington merit}{{3.4.1}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The merit function.}}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Digging Deeper}{33}}
\newlabel{fig:bevington residuals lines}{{3.4.1}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Another look at the merit function showing the primary error ellipse.}}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Whisker plot showing 250 randomly sampled solutions.}}{34}}
\newlabel{fig:bev whisker}{{3.7}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces The solution parameters expressed as normal distributions.}}{35}}
\newlabel{tab:dev normal}{{3.4}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Comparing samples to ideal normal distribution.}}{35}}
\newlabel{tab:bev census}{{3.5}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Scatter plot showing sampling of solutions.}}{36}}
\newlabel{fig:bev scatter}{{3.8}{36}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Modal Example II: Other Methods}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:normal II}{{4.1}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Normal Equations from Vectors}{37}}
\newlabel{eq:bevington axeb}{{4.1}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Composing the normal equations}{38}}
\newlabel{eq:det again}{{4.2}{39}}
\newlabel{eq:bevington:soln:vectors}{{4.3}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Singular Value Decomposition}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Computing the SVD}{39}}
\@writefile{toc}{\contentsline {subsubsection}{Singular values}{40}}
\@writefile{toc}{\contentsline {subsubsection}{Domain matrix}{41}}
\newlabel{eq:decompwx}{{4.4}{41}}
\newlabel{eq:vrot}{{4.5}{41}}
\newlabel{eq:costtheory}{{4.6}{41}}
\@writefile{toc}{\contentsline {subsubsection}{Codomain matrix}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Error terms}{42}}
\newlabel{sssec:archetype viz}{{4.2.1}{43}}
\@writefile{toc}{\contentsline {subsubsection}{Visualization}{43}}
\newlabel{eq:alpha}{{4.2.1}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The solution vector is the mixture of $u_{1}$ and $u_{2}$ which eliminates error.}}{45}}
\newlabel{fig:u min}{{4.1}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Measurement space for the Bevington example}}{46}}
\newlabel{fig:bevington codomain}{{4.2}{46}}
\newlabel{tab:bevington poles}{{4.2.1}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The column vectors of $ \mathbf  {U}^{\mathrm  {  }} $.}}{46}}
\newlabel{eq:archetype:data vector}{{4.2.1}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Minimization occurs in the codomain.}}{47}}
\newlabel{fig:bevington codomain with data}{{4.3}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Data vector resolved into range and null space components.}}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Decomposing $\left \delimiter 69645069 {\color {red} {r}} = {\color {red} {T_{ \mathcal  {N} }}} \right \delimiter 86422285 _{2}^{2}$ into residual error terms $r_{k}^2$ of table 4.3\hbox {}.}}{47}}
\newlabel{tab:bevington usv block}{{4.2.1}{48}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Singular value decomposition for the system matrix $\A {}$.}}{48}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces A summary of the residual errors and their contributions to $\left \delimiter 69645069 {\color {red} {r}} \right \delimiter 86422285 _{2}$.}}{48}}
\newlabel{tab:bev r decomposition}{{4.3}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}$ \textbf  {Q}^{\mathrm  {  }}  \textbf  {R}^{\mathrm  {  }} $ Decomposition}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Computing the QR Decomposition}{49}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Modal Example III: Observations}{51}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Invariances}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Translation Invariance}{51}}
\@writefile{toc}{\contentsline {subsubsection}{Translation}{51}}
\@writefile{toc}{\contentsline {subsubsection}{Demonstrations}{52}}
\newlabel{eq:translation:dot products}{{5.1.1}{52}}
\@writefile{toc}{\contentsline {subsubsection}{Computations}{52}}
\newlabel{eq:bev:prediction}{{5.1}{52}}
\@writefile{toc}{\contentsline {subsubsection}{Visuals}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Solution after translation along $x$ axis: the slope is invariant.}}{53}}
\newlabel{fig:bevington translate soln v data}{{5.1}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Reflection Invariance}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fitting To Higher Orders}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Scatter plot of residual errors.}}{54}}
\newlabel{fig:bevington translate residuals}{{5.2}{54}}
\newlabel{fig:bevington translate merit}{{5.1.1}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The merit function after translation.}}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The merit function for the learning curve showing the minimum and the value.}}{55}}
\newlabel{fig:learn:merit}{{5.4}{55}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{II}Applications:\\Nonlinear Problems}{57}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Linearization}{59}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Powers Laws and Exponentials}{59}}
\newlabel{eq:learning curve}{{6.1}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Learning Curve}{59}}
\newlabel{eq:learn merit surface}{{6.2}{59}}
\newlabel{tab:data learn}{{6.2}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Problem Statement}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Merit function for the learning curve in solution space.}}{61}}
\newlabel{fig:learn merit naked}{{6.1}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Solution}{61}}
\newlabel{eq:learn:a}{{6.2.2}{61}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Problem statement for learning curve.}}{62}}
\newlabel{tab:learn:problem statement}{{6.1}{62}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces The simultaneous conditions defining $\nabla M(a,b) = 0$.}}{62}}
\newlabel{tab:learn:gradient}{{6.2}{62}}
\newlabel{eq:learn:merit 1d}{{6.3}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Merit function constrained to one parameter $b$.}}{63}}
\newlabel{fig:learn:one d}{{6.2}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Results}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}What Not To Do}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Logarithmic Transform}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Merit function for the learning curve in solution space showing the constrained $a$ parameter as a dotted line.}}{64}}
\newlabel{fig:learn:constrained merit}{{6.3}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Solution for equations \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 6.2\hbox {}\unskip \@@italiccorr )}} and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 6.3\hbox {}\unskip \@@italiccorr )}} using data in table 6.2\hbox {}.}}{64}}
\newlabel{fig:learn: data v soln}{{6.4}{64}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Results for learning curve analysis.}}{65}}
\newlabel{tab:bevington solution}{{6.3}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The residual errors in figure 6.4\hbox {}.}}{65}}
\newlabel{fig:learn:residuals}{{6.5}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Minimization of the merit function for the learning curve.}}{66}}
\newlabel{fig:learn:merit}{{6.6}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Linear Transformation}{66}}
\newlabel{eq:learn:soln:faux}{{6.4}{67}}
\newlabel{eq:learn:merit:min}{{6.5}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{67}}
\newlabel{fig:learn:merit}{{6.7}{67}}
\newlabel{ssec:RTF}{{6.3.3}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Reflection Test Fails}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Radioactive Decay}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Theory}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Problem Statement}{68}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Problem statement for radioactive decay.}}{68}}
\newlabel{tab:bevington inputs}{{6.4}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Results}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{69}}
\newlabel{fig:learn:merit}{{6.8}{69}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Logarithmic scaling distorts errors.}}{70}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Results for radioactive decay.}}{71}}
\newlabel{tab:bevington solution}{{6.6}{71}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Population Growth}{73}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Model}{73}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Problem statement for population model with linear and exponential growth.}}{74}}
\newlabel{tab:census problem statement}{{7.1}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Problem Statement}{74}}
\newlabel{eq:census:error}{{7.3}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Data}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Example}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Polynomials}{74}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Data v. prediction.}}{75}}
\newlabel{tab:census results}{{7.2}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Residual error for \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 7.3\hbox {}\unskip \@@italiccorr )}} with $a_{1}$, $a_{2}$, and $a_{3}$ at optimal values.}}{75}}
\newlabel{fig:census:line:residual error:wide}{{7.1}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Residual error for $a_{1}$ and $a_{2}$ fixed at optimal values.}}{76}}
\newlabel{fig:census:line:residual error:wide}{{7.2}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Solution plotted against data.}}{76}}
\newlabel{fig:census:data v soln}{{7.3}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Scatterplot of residual errors.}}{77}}
\newlabel{fig:census:scatterplot}{{7.4}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces The merit function showing least squares solution.}}{77}}
\newlabel{fig:census:merit}{{7.5}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces The merit function showing least squares solution and the null cline.}}{78}}
\newlabel{fig:census:merit}{{7.6}{78}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Results for census analysis}}{78}}
\newlabel{tab:results census}{{7.3}{78}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Fitting the census data with low order polynomials.}}{79}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces Fitting the census data with higher order polynomials.}}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Total error $r^{\mathrm  {*}}r$ by order of fit.}}{81}}
\@writefile{lot}{\contentsline {table}{\numberline {7.6}{\ignorespaces Projections, by order of fit, for population in 2010.}}{81}}
\@writefile{toc}{\contentsline {part}{\hbox to 20pt{III}\ \ Backmatter}{83}}
\bibcite{Bellman1997}{1}
\bibcite{Bevington}{2}
\bibcite{Chan2007}{3}
\bibcite{Demmel1997}{4}
\bibcite{Golub1996}{5}
\bibcite{Higham2008}{6}
\bibcite{Horn1990}{7}
\bibcite{Horn1991}{8}
\bibcite{Mercer}{9}
\bibcite{Laub2005}{10}
\bibcite{Meyer2000}{11}
\bibcite{Strang2005}{12}
\bibcite{Trefethen2000}{13}
\bibcite{Eric}{14}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{85}}
