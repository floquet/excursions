\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces The residual error $\left \delimiter 69645069 r \right \delimiter 86422285 _{2}$ given in \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 1.5\hbox {}\unskip \@@italiccorr )}}.}}{5}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Scalar function $\phi $ and approximations.}}{6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Decomposing the data vector.}}{16}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Projections of the data vector.}}{19}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Data vector resolved as $b = {\color {blue} {b_{ \mathcal {R} }}} + {\color {red} {b_{ \mathcal {N} }}}$.}}{20}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Data vector resolved as $b + {\color {red} {b_{ \mathcal {N} }}} = {\color {blue} {b_{ \mathcal {R} }}}$.}}{20}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Reflecting the data points through the solution curve.}}{21}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Measuring the temperature of a bar.}}{26}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Solution plotted against data with residual errors shown in red.}}{33}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Scatter plot of residual errors.}}{34}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Scatter plot of residual errors with data points connected.}}{34}
\contentsline {figure}{\numberline {2.10}{\ignorespaces The merit function.}}{35}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Another look at the merit function showing the primary error ellipse (black) and contour levels (gray).}}{36}
\contentsline {figure}{\numberline {2.12}{\ignorespaces Whisker plot showing 250 randomly sampled solutions.}}{37}
\contentsline {figure}{\numberline {2.13}{\ignorespaces Scatter plot showing sampling of solutions.}}{38}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The solution vector is the mixture of $u_{1}$ and $u_{2}$ which eliminates error.}}{47}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Measurement space for the Bevington example}}{48}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Minimization occurs in the codomain.}}{49}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Data vector $T = {{\color {blue} {T_{ \mathcal {R} }}}} + {{\color {red} {T_{ \mathcal {N} }}}}$ resolved into range and null space components as in figure 2.3\hbox {}.}}{49}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Decomposing $\left \delimiter 69645069 {\color {red} {r}} = {\color {red} {T_{ \mathcal {N} }}} \right \delimiter 86422285 _{2}^{2}$ into residual error terms $r_{k}^2$ of table 3.3\hbox {}.}}{49}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Solution after translation along $x$ axis: the slope is invariant.}}{61}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Scatter plot of residual errors.}}{62}
\contentsline {figure}{\numberline {5.3}{\ignorespaces The merit function after translation.}}{63}
\contentsline {figure}{\numberline {5.4}{\ignorespaces The merit function for the learning curve showing the minimum and the value.}}{63}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Parallel lines and the least squares solution.}}{66}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces A slice of a face-centered cubic lattice showing a single crystal.}}{71}
\contentsline {figure}{\numberline {8.2}{\ignorespaces Simulation output showing atomic shades shaded by potential energy.}}{72}
\contentsline {figure}{\numberline {8.3}{\ignorespaces Full data set showing inset.}}{73}
\contentsline {figure}{\numberline {8.4}{\ignorespaces Sample data set showing fit parameters.}}{73}
\contentsline {figure}{\numberline {8.5}{\ignorespaces Solutions for three data sets.}}{77}
\contentsline {figure}{\numberline {8.6}{\ignorespaces Apex angles displayed in table 8.7\hbox {}.}}{80}
\contentsline {figure}{\numberline {8.7}{\ignorespaces Merit functions for the three data sets.}}{81}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {10.1}{\ignorespaces Stitching local maps together to form a global map.}}{85}
\contentsline {figure}{\numberline {10.2}{\ignorespaces The ideal potential function showing five measurement zones and four overlap bands.}}{87}
\contentsline {figure}{\numberline {10.3}{\ignorespaces Waterfall diagram showing discretization within measurement zones with left and right zone overlaps.}}{87}
\contentsline {figure}{\numberline {10.4}{\ignorespaces Stitching unifies the data.}}{88}
\contentsline {figure}{\numberline {10.5}{\ignorespaces A set of piston adjustments which restores continuity across the domain.}}{88}
\contentsline {figure}{\numberline {10.6}{\ignorespaces Looking at the merit function on the $p_{2} - p_{3}$ axis.}}{93}
\contentsline {figure}{\numberline {10.7}{\ignorespaces Pistons from the solution and pistons used to create the data}}{94}
\contentsline {figure}{\numberline {10.8}{\ignorespaces A set of tilt adjustments which restores continuity of the gradient across the domain.}}{95}
\contentsline {figure}{\numberline {10.9}{\ignorespaces A function and its gradient.}}{95}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {12.1}{\ignorespaces Merit function for the learning curve in solution space.}}{103}
\contentsline {figure}{\numberline {12.2}{\ignorespaces Merit function constrained to one parameter $b$.}}{105}
\contentsline {figure}{\numberline {12.3}{\ignorespaces Merit function for the learning curve in solution space showing the constrained $a$ parameter as a dotted line.}}{106}
\contentsline {figure}{\numberline {12.4}{\ignorespaces Solution for equations \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 12.2\hbox {}\unskip \@@italiccorr )}} and \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 12.3\hbox {}\unskip \@@italiccorr )}} using data in table 12.2\hbox {}.}}{106}
\contentsline {figure}{\numberline {12.5}{\ignorespaces The residual errors in figure 12.4\hbox {}.}}{107}
\contentsline {figure}{\numberline {12.6}{\ignorespaces Minimization of the merit function for the learning curve.}}{108}
\contentsline {figure}{\numberline {12.7}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{109}
\contentsline {figure}{\numberline {12.8}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{111}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {13.1}{\ignorespaces Residual error for \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 13.3\hbox {}\unskip \@@italiccorr )}} with $a_{1}$, $a_{2}$, and $a_{3}$ at optimal values.}}{117}
\contentsline {figure}{\numberline {13.2}{\ignorespaces Residual error for $a_{1}$ and $a_{2}$ fixed at optimal values.}}{118}
\contentsline {figure}{\numberline {13.3}{\ignorespaces Solution plotted against data.}}{118}
\contentsline {figure}{\numberline {13.4}{\ignorespaces Scatterplot of residual errors.}}{119}
\contentsline {figure}{\numberline {13.5}{\ignorespaces The merit function showing least squares solution.}}{119}
\contentsline {figure}{\numberline {13.6}{\ignorespaces The merit function showing least squares solution and the null cline.}}{120}
\contentsline {figure}{\numberline {13.7}{\ignorespaces Total error $r^{\mathrm {*}}r$ by order of fit.}}{123}
\addvspace {10\p@ }
