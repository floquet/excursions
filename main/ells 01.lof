\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces The residual error $\left \delimiter 69645069 r \right \delimiter 86422285 _{2}$ given in \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 1.5\hbox {}\unskip \@@italiccorr )}}.}}{5}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Scalar function $\phi $ and approximations.}}{6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Decomposing the data vector.}}{16}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Projections of the data vector.}}{19}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Data vector resolved as $b = {\color {blue} {b_{ \mathcal {R} }}} + {\color {red} {b_{ \mathcal {N} }}}$.}}{20}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Data vector resolved as $b + {\color {red} {b_{ \mathcal {N} }}} = {\color {blue} {b_{ \mathcal {R} }}}$.}}{20}
\contentsline {figure}{\numberline {2.5}{\ignorespaces The merit function for the learning curve showing the minimum and the value.}}{21}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Measuring the temperature of a bar.}}{28}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Solution plotted against data with residual errors shown in red.}}{35}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Scatter plot of residual errors.}}{36}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Scatter plot of residual errors with data points connected.}}{36}
\contentsline {figure}{\numberline {3.5}{\ignorespaces The merit function.}}{37}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Another look at the merit function showing the primary error ellipse.}}{38}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Whisker plot showing 250 randomly sampled solutions.}}{38}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Scatter plot showing sampling of solutions.}}{40}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The solution vector is the mixture of $u_{1}$ and $u_{2}$ which eliminates error.}}{46}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Measurement space for the Bevington example}}{48}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Minimization occurs in the codomain.}}{49}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Data vector resolved into range and null space components.}}{49}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Decomposing $\left \delimiter 69645069 {\color {red} {r}} = {\color {red} {T_{ \mathcal {N} }}} \right \delimiter 86422285 _{2}^{2}$ into residual error terms $r_{k}^2$ of table 4.3\hbox {}.}}{49}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Solution after translation along $x$ axis: the slope is invariant.}}{57}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Scatter plot of residual errors.}}{58}
\contentsline {figure}{\numberline {6.3}{\ignorespaces The merit function after translation.}}{59}
\contentsline {figure}{\numberline {6.4}{\ignorespaces The merit function for the learning curve showing the minimum and the value.}}{59}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Parallel lines and the least squares solution.}}{62}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces A slice of a face-centered cubic lattice showing a single crystal.}}{67}
\contentsline {figure}{\numberline {9.2}{\ignorespaces Simulation output showing atomic shades shaded by potential energy.}}{68}
\contentsline {figure}{\numberline {9.3}{\ignorespaces Full data set showing inset.}}{69}
\contentsline {figure}{\numberline {9.4}{\ignorespaces Sample data set showing fit parameters.}}{69}
\contentsline {figure}{\numberline {9.5}{\ignorespaces Solutions for three data sets.}}{73}
\contentsline {figure}{\numberline {9.6}{\ignorespaces Apex angles displayed in table 9.7\hbox {}.}}{76}
\contentsline {figure}{\numberline {9.7}{\ignorespaces Merit functions for the three data sets.}}{77}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {11.1}{\ignorespaces Stitching local maps together to form a global map.}}{81}
\contentsline {figure}{\numberline {11.2}{\ignorespaces The ideal potential function showing five measurement zones and four overlap bands.}}{83}
\contentsline {figure}{\numberline {11.3}{\ignorespaces Waterfall diagram showing discretization within measurement zones with left and right zone overlaps.}}{83}
\contentsline {figure}{\numberline {11.4}{\ignorespaces Stitching unifies the data.}}{84}
\contentsline {figure}{\numberline {11.5}{\ignorespaces A set of piston adjustments which restores continuity across the domain.}}{84}
\contentsline {figure}{\numberline {11.6}{\ignorespaces Looking at the merit function on the $p_{2} - p_{3}$ axis.}}{89}
\contentsline {figure}{\numberline {11.7}{\ignorespaces Pistons from the solution and pistons used to create the data}}{90}
\contentsline {figure}{\numberline {11.8}{\ignorespaces A set of tilt adjustments which restores continuity of the gradient across the domain.}}{91}
\contentsline {figure}{\numberline {11.9}{\ignorespaces A function and its gradient.}}{91}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {13.1}{\ignorespaces Merit function for the learning curve in solution space.}}{99}
\contentsline {figure}{\numberline {13.2}{\ignorespaces Merit function constrained to one parameter $b$.}}{101}
\contentsline {figure}{\numberline {13.3}{\ignorespaces Merit function for the learning curve in solution space showing the constrained $a$ parameter as a dotted line.}}{102}
\contentsline {figure}{\numberline {13.4}{\ignorespaces Solution for equations \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 13.2\hbox {}\unskip \@@italiccorr )}} and \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 13.3\hbox {}\unskip \@@italiccorr )}} using data in table 13.2\hbox {}.}}{102}
\contentsline {figure}{\numberline {13.5}{\ignorespaces The residual errors in figure 13.4\hbox {}.}}{103}
\contentsline {figure}{\numberline {13.6}{\ignorespaces Minimization of the merit function for the learning curve.}}{104}
\contentsline {figure}{\numberline {13.7}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{105}
\contentsline {figure}{\numberline {13.8}{\ignorespaces The merit function for the learning curve showing the minimum and the value .}}{107}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {14.1}{\ignorespaces Residual error for \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 14.3\hbox {}\unskip \@@italiccorr )}} with $a_{1}$, $a_{2}$, and $a_{3}$ at optimal values.}}{113}
\contentsline {figure}{\numberline {14.2}{\ignorespaces Residual error for $a_{1}$ and $a_{2}$ fixed at optimal values.}}{114}
\contentsline {figure}{\numberline {14.3}{\ignorespaces Solution plotted against data.}}{114}
\contentsline {figure}{\numberline {14.4}{\ignorespaces Scatterplot of residual errors.}}{115}
\contentsline {figure}{\numberline {14.5}{\ignorespaces The merit function showing least squares solution.}}{115}
\contentsline {figure}{\numberline {14.6}{\ignorespaces The merit function showing least squares solution and the null cline.}}{116}
\contentsline {figure}{\numberline {14.7}{\ignorespaces Total error $r^{\mathrm {*}}r$ by order of fit.}}{119}
\addvspace {10\p@ }
