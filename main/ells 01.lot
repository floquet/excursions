\addvspace {10\p@ }
\contentsline {table}{\numberline {0.1}{\ignorespaces Matrix manipulations for $\A {*}$ and $\A {T}$.}}{ix}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces The Fundamental Theorem of Linear Algebra}}{10}
\contentsline {table}{\numberline {2.2}{\ignorespaces The Fundamental Theorem of Linear Algebra in pictures}}{11}
\contentsline {table}{\numberline {2.3}{\ignorespaces Dimensions of the fundamental subspaces for $ \A {} \in \mathbb {C}^{m \times n}_{\rho }$.}}{12}
\contentsline {table}{\numberline {2.4}{\ignorespaces Orthonormal spans for the invariant subspaces.}}{13}
\contentsline {table}{\numberline {2.5}{\ignorespaces Fundamental Projectors using the pseudoinverse.}}{18}
\contentsline {table}{\numberline {2.6}{\ignorespaces Fundamental Projectors using domain matrices.}}{19}
\contentsline {table}{\numberline {2.7}{\ignorespaces Reflecting the data for $\A {}x - T = r$.}}{21}
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Problem statement for least squares solution for a single line.}}{24}
\contentsline {table}{\numberline {3.2}{\ignorespaces Rewriting the equations $y=mx+b$ as a linear system.}}{26}
\contentsline {table}{\numberline {3.3}{\ignorespaces Least squares solution for three distinct lines as the parameter $m$ varies from 0 to $\infty $.}}{29}
\contentsline {table}{\numberline {3.4}{\ignorespaces Least squares solution for three distinct lines as the parameter $m$ varies from 0 to $\infty $.}}{30}
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Problem statement for linear regression.}}{38}
\contentsline {table}{\numberline {4.2}{\ignorespaces Raw data and results.}}{39}
\contentsline {table}{\numberline {4.3}{\ignorespaces Results for linear regression.}}{40}
\contentsline {table}{\numberline {4.4}{\ignorespaces The solution parameters expressed as normal distributions.}}{47}
\contentsline {table}{\numberline {4.5}{\ignorespaces Comparing samples to ideal normal distribution.}}{48}
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces The column vectors of $ \mathbf {U}^{\mathrm { }} $.}}{60}
\contentsline {table}{\numberline {5.2}{\ignorespaces Singular value decomposition for the system matrix $\A {}$.}}{60}
\contentsline {table}{\numberline {5.3}{\ignorespaces A summary of the residual errors and their contributions to $\left \delimiter 69645069 {\color {red} {r}} \right \delimiter 86422285 _{2}$.}}{61}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {7.1}{\ignorespaces Relating the pair index $\mu $ to the indices $j$ and $k$.}}{74}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {9.1}{\ignorespaces Data sets and basic results}}{82}
\contentsline {table}{\numberline {9.2}{\ignorespaces Problem statement for grain identification by rows (coupled linear regression).}}{85}
\contentsline {table}{\numberline {9.3}{\ignorespaces Point membership in data sets shown in figure 9.1\hbox {}.}}{86}
\contentsline {table}{\numberline {9.4}{\ignorespaces Excerpted data set.}}{87}
\contentsline {table}{\numberline {9.5}{\ignorespaces Least squares results for three axes.}}{87}
\contentsline {table}{\numberline {9.6}{\ignorespaces Intermediate results: angles for the axes.}}{87}
\contentsline {table}{\numberline {9.7}{\ignorespaces Final results: apex angle measurements}}{87}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {11.1}{\ignorespaces The input data in continuous and discrete form.}}{94}
\contentsline {table}{\numberline {11.2}{\ignorespaces Sample showing two zones with overlap.}}{94}
\contentsline {table}{\numberline {11.3}{\ignorespaces Measurements displaying the connection between overlap bands in figure 11.3\hbox {}.}}{97}
\contentsline {table}{\numberline {11.4}{\ignorespaces Computation of the zone shift values.}}{97}
\contentsline {table}{\numberline {11.5}{\ignorespaces Computation of the zone shift values.}}{98}
\contentsline {table}{\numberline {11.6}{\ignorespaces Input data}}{98}
\contentsline {table}{\numberline {11.7}{\ignorespaces Problem statement for linear regression.}}{99}
\contentsline {table}{\numberline {11.8}{\ignorespaces Results for stitching with piston.}}{100}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {13.1}{\ignorespaces Problem statement for linear regression.}}{112}
\contentsline {table}{\numberline {13.2}{\ignorespaces Results for best circle}}{113}
\addvspace {10\p@ }
\contentsline {table}{\numberline {14.1}{\ignorespaces Problem statement for learning curve.}}{118}
\contentsline {table}{\numberline {14.2}{\ignorespaces The simultaneous conditions defining $\nabla M(a,b) = 0$.}}{118}
\contentsline {table}{\numberline {14.3}{\ignorespaces Results for learning curve analysis.}}{121}
\contentsline {table}{\numberline {14.4}{\ignorespaces Problem statement for radioactive decay.}}{124}
\contentsline {table}{\numberline {14.5}{\ignorespaces Logarithmic scaling distorts errors.}}{126}
\contentsline {table}{\numberline {14.6}{\ignorespaces Results for radioactive decay.}}{127}
\addvspace {10\p@ }
\contentsline {table}{\numberline {15.1}{\ignorespaces Problem statement for population model with linear and exponential growth.}}{130}
\contentsline {table}{\numberline {15.2}{\ignorespaces Data v. prediction.}}{131}
\contentsline {table}{\numberline {15.3}{\ignorespaces Results for census analysis}}{134}
\contentsline {table}{\numberline {15.4}{\ignorespaces Fitting the census data with low order polynomials.}}{135}
\contentsline {table}{\numberline {15.5}{\ignorespaces Fitting the census data with higher order polynomials.}}{136}
\contentsline {table}{\numberline {15.6}{\ignorespaces Fitting the census data with low order polynomials.}}{137}
\contentsline {table}{\numberline {15.7}{\ignorespaces Fitting the census data with higher order polynomials.}}{138}
\contentsline {table}{\numberline {15.8}{\ignorespaces Projections, by order of fit, for population in 2010.}}{139}
\addvspace {10\p@ }
